{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "2ca7ac99-155b-4138-ae9b-5891e96fe2c9",
    "_uuid": "a5b0be51-1bfe-47b4-8f63-8e5f59aab120"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import strptime\n",
    "import string\n",
    "import re\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "665c928a-f109-47db-b0ce-f566af163b09",
    "_uuid": "2e1bf493-f88e-4ef3-81d3-bd187a227dbe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "cd965589-0f0f-4f68-93b0-f654b462c758",
    "_uuid": "2828fe0d-55f4-4399-878f-fc3402ce5a2f"
   },
   "outputs": [],
   "source": [
    "sharks = pd.read_excel('/kaggle/input/shark-attacks-gsaf5/GSAF5 (sharks).xls', sort =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "4288dabe-0bc2-449f-a01a-e5a0b7b911e8",
    "_uuid": "1157283f-9d03-4d7f-b619-2136f8068813"
   },
   "outputs": [],
   "source": [
    "sharks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "4fbbcb43-0638-440e-b03e-326afb27339a",
    "_uuid": "85f56239-12e1-47a7-99f5-562a68b12d20"
   },
   "outputs": [],
   "source": [
    "sharks.dropna(subset = [\"Date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "1fd12490-052a-403f-a89b-d7ba230ae1b0",
    "_uuid": "4e416aa1-720c-412e-a039-a5e50428d4ea"
   },
   "outputs": [],
   "source": [
    "sharks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "16a1f070-a25b-40f2-b77d-ed039c1e4d0d",
    "_uuid": "9bf2058b-5ac7-4212-8fce-d157a3a4b84b"
   },
   "outputs": [],
   "source": [
    "sharks = sharks.drop(columns = [ 'Unnamed: 23', 'href formula', 'pdf', 'Case Number.1','Case Number.2', 'original order'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "067063cf-c972-4237-aabf-015c4a02ef9d",
    "_uuid": "944f449b-0e54-4f31-8621-07927948a99d"
   },
   "source": [
    "## Clean 'Country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "55e22d2b-f3c2-4d65-8e03-e5237bdbafca",
    "_uuid": "66e9be95-ffb4-4a2c-934f-2ab6a7b26024"
   },
   "outputs": [],
   "source": [
    "#set(sharks.Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "c9a30c8d-fc72-4fc3-a35c-3749fb80e2a3",
    "_uuid": "e8b380b5-dec8-4907-ad9c-5adcb4b8e530"
   },
   "outputs": [],
   "source": [
    "sharks['Country2'] = [str(x).strip().upper() for x in sharks['Country']]    \n",
    "sharks['Country'] = sharks['Country2']\n",
    "sharks = sharks.drop(columns = ['Country2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "8a3d82fc-cd21-4d03-93c1-f7609341e5ed",
    "_uuid": "499418d6-6778-4bc0-9686-13ab8c11fb35"
   },
   "outputs": [],
   "source": [
    "#sharks[sharks['Country'].str.contains(r'.+\\?')]\n",
    "sharks['Country'] = sharks['Country'].str.replace('?', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "0fda8f6a-04f1-441c-b45b-73ff875770c9",
    "_uuid": "3418b139-933e-42f2-99af-7214c0828f78"
   },
   "outputs": [],
   "source": [
    "sharks.loc[sharks['Country'] == 'COAST OF AFRICA', 'Country'] = 'AFRICA'\n",
    "sharks.loc[sharks['Country'] == 'CEYLON (SRI LANKA)', 'Country'] = 'CEYLON'\n",
    "sharks.loc[sharks['Country'] == 'REUNION', 'Country'] = 'REUNION ISLAND'\n",
    "sharks.loc[sharks['Country'] == 'MALDIVES', 'Country'] = 'MALDIVE ISLANDS'\n",
    "sharks.loc[sharks['Country'] == 'UNITED ARAB EMIRATES (UAE)', 'Country'] = 'UNITED ARAB EMIRATES'\n",
    "sharks.loc[sharks['Country'] == 'ST HELENA, British overseas territory', 'Country'] = 'ST. HELENA'\n",
    "sharks.loc[sharks['Country'] == 'COLUMBIA', 'Country'] = 'COLOMBIA'\n",
    "sharks.loc[sharks['Country'] == 'SOUTHWEST PACIFIC OCEAN', 'Country'] = 'SOUTH PACIFIC OCEAN'\n",
    "sharks.loc[sharks['Country'] == 'FEDERATED STATES OF MICRONESIA', 'Country'] = 'MICRONESIA'\n",
    "sharks.loc[sharks['Country'] == 'TOBAGO', 'Country'] = 'TRINIDAD & TOBAGO'\n",
    "sharks.loc[sharks['Country'] == 'WESTERN SAMOA', 'Country'] = 'SAMOA'\n",
    "sharks.loc[sharks['Country'] == 'MID-PACIFC OCEAN', 'Country'] = 'MID PACIFIC OCEAN' \n",
    "sharks.loc[sharks['Country'] == 'SOLOMON ISLANDS', 'Country'] = 'SOLOMON ISLANDS / VANUATU'\n",
    "sharks.loc[sharks['Country'] == 'VANUATU', 'Country'] = 'SOLOMON ISLANDS / VANUATU'\n",
    "sharks.loc[sharks['Country'] == 'RED SEA / INDIAN OCEAN', 'Country'] = 'RED SEA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9fd55ade-6115-4244-a9be-0fc61bc40e82",
    "_uuid": "8f07523a-6521-4c7c-902a-017d33c633eb"
   },
   "source": [
    "## Clean 'Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "ba38e8b6-a84a-440b-b869-147851076bc3",
    "_uuid": "176e9774-fe04-4948-9233-55cc5f804299"
   },
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Year'].between(1, 1600, inclusive=True) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "d46905fd-f920-426c-aa0f-3870e3b76a21",
    "_uuid": "054e910b-f5ce-4b7b-8264-4046a2762fd0"
   },
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Date'].str.contains(r'.+B\\.C\\.') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "7b6f3ea4-64ea-46b5-8244-1f358227eb00",
    "_uuid": "ee224b74-8647-400f-8c01-376d6e0f51c5"
   },
   "outputs": [],
   "source": [
    "sharks['Year2'] = sharks['Date'].str.extract(r'([0-9]{4})')\n",
    "sharks.loc[sharks['Year']==0, 'Year'] = sharks['Year2']\n",
    "\n",
    "#some of the years extracted above are approximated. ie, 'Before 1905' is too vague to estimate; I've taken 1905 as the year. This could vary in degree of incorrectness- actual may be 1904 or 1704. \n",
    "#possibly should be dropped altogether- will decide when viz of year is viewed\n",
    "\n",
    "sharks.loc[sharks['Year'].isna() == True, 'Year'] = sharks['Year2']\n",
    "sharks = sharks.drop(columns = ['Year2'])\n",
    "sharks.loc[sharks['Date'] == 'World War II', 'Year'] = '1940'\n",
    "sharks['Year'] = sharks['Year'].replace(\"'\", '')\n",
    "sharks = sharks[sharks['Year'].isna() == False]\n",
    "sharks['Year'] = sharks['Year'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "d82d57b6-94b7-44a5-adcf-abc4b1213586",
    "_uuid": "d03976a0-09c6-40c9-8e00-e11f43dbb044"
   },
   "outputs": [],
   "source": [
    "#sharks['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5cd57c0-7ce7-4f4b-9e56-668e368afd04",
    "_uuid": "bb8b19ed-2c24-49d2-9785-8f620d834fbb"
   },
   "source": [
    "## Clean 'Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "7a774e46-f429-47b5-bb19-92fff19355ed",
    "_uuid": "662830d8-76b0-444b-9db5-a3f7f49a9183"
   },
   "outputs": [],
   "source": [
    "sharks['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "1f854b6b-a70a-4041-9938-66a24c58c03c",
    "_uuid": "3d982e39-b5b1-4ae9-a0ef-a9aca6193672"
   },
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Case Number'] != '2020.04.29']\n",
    "sharks['Type'][sharks['Type'].isna() == True] = 'Unprovoked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "f0263d07-ceab-49fa-a64f-01ee5d64d23e",
    "_uuid": "ec548b19-be5f-4b14-9cdb-0dbcce675888"
   },
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Type'] != 'Invalid'] #if invalid, will only confuse data\n",
    "sharks.loc[sharks['Type'] == 'Boat', 'Type'] = 'Watercraft' # will actually switch to 'Watercraft' for consistency\n",
    "sharks = sharks[sharks['Type'] != 'Under investigation'] # there is only one so just going to drop it\n",
    "sharks = sharks[sharks['Type'] != 'Unconfirmed'] # there is only one so just going to drop it- missing much info so don't want to coerce into unprovoked\n",
    "sharks.loc[(sharks['Type'].isna() == True) & (sharks['Activity'] == 'Wreck of a sampam'), 'Type'] = 'Sea Disaster'\n",
    "sharks.loc[sharks['Type'] == 'Unverified', 'Type'] = 'Unprovoked' #only 1; acting as if legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "debaeee0-9fbf-4604-a05f-53af7138c4c1",
    "_uuid": "6ba90c4d-70a3-49e4-b7a7-4396775e82a2"
   },
   "source": [
    "## Clean 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "39ba44fb-c2d9-418c-ac59-044230fa5443",
    "_uuid": "46ff880c-8e06-4a16-b4f3-ede4d069bbfa"
   },
   "outputs": [],
   "source": [
    "set(sharks['Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "fd85b087-bc8e-477d-bd9f-8267d3c885c5",
    "_uuid": "98c857cc-f333-4318-95bf-15324b0b16e9"
   },
   "outputs": [],
   "source": [
    "#set(sharks['Activity'][(sharks['Activity'].str.contains(r'(swim|stand|wading|Swim|Stand|Wading)')) & (~sharks['Activity'].str.contains(r'(Fish|fish|apsize|shark|Shark|porpoise|oyster|whale|dolphin|perlemoen|abalone|slaughterhouse)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False))])\n",
    "sharks['Activity_clean'] = ''\n",
    "# includes: (Attempt|Attack|harpooned|dead|Catching|Chumming|captured|force-feed|Kiss|feeding|photographing|Dragging|filming|Feeding|Feeling|Filming|Finning|Fell|Fishing|Gaffing|Grab|Hand feed|Harass|Harpoon|Hauling|Help|Hoist|Hold|Hunt|Inspect|Investigat|landed on|Killing|fishing|Measuring|Moving|Netting|Observing|Petting|Photographing|Picking|Pulling|foot|hand|Removing|Restrain|Reviv|catch|Shooting|Grab|Slap|lasso|pulled|shot|Spearing|Standing|stepped|Stuffing|feeding|Swimming with|Tagging|Teasing|Testing|Thrashing|Touching|netted|Watching|Wrangling|)\n",
    "\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(collided|[C|c]apsize|aground|[S|s]hipwreck|210-ton brig|[W|w]reck|[D|d]isaster|[A|a]ccident|founder|crash|overturn|stove|sank)', na=False)) | (sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_clean'] = 'Sea disaster/capsize/wreck/accident'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'( fish|[F|f]ish|[C|c]ollecting|[C|c]lam|porpoise|whale|dolphin|perlemoen|[A|a]balone|crayfish|slaughterhouse|bait|[T|t]rochus|opihi|squid|shad|bluefish|net|[C|c]rab|conch|beche-de-mer|trepang|tuna|shellfish|[L|l]obster|[S|s]hrimp|[P|p]rawn|meat|turtle|seal|[O|o]yster|[A|a]ngling|sardine|bichiques|[C|c]hum|[S|s]almon|[F|f]lounder|sea urchins|sponges)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)) & (~sharks['Activity'].str.contains(r'Hawaiian brig|Observing', na=False)), 'Activity_clean'] = 'Fishing / Proximity to Sea Life'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(SUP|[Ss]wim|[Ss]ta[mn]d|[Ww]ad[e|ing]|[Bb]athing|[D|d]iv[e|ing]|[S|s]urf|[W|w]alk|[S|s]it|[B|b]oard|Playing|Snorkeling|Floating|[F|f]eet|Cling|Clean|Lying|Adrift|Kneeling|Holding|Paddling|Splash|Wash|[F|f]ell|Crouch|banana|Filming$|documentary|Jump|Rest|Sitting|Tread|[S|s]ki)')) & (~sharks['Activity'].str.contains(r'(Chasing|[C|c]apsize|aground|[S|s]hipwreck|210-ton brig|[W|w]reck|[D|d]isaster|[A|a]ccident|feeding| fish|[F|f]ish|[C|c]lam|porpoise|oyster|whale|dolphin|perlemoen|[A|a]balone|crayfish|slaughterhouse|bait|trochus|opihi|squid|bluefish|[C|c]rab|conch|beche-de-mer|trepang|tuna|shellfish|[L|l]obster|[S|s]hrimp|meat|turtle|seal|[O|o]yster|[C|c]hum|[S|s]almon|sea urchins|sponges|Feeding|holding shark|putting hand|shark tank|stepped on|grabbed shark)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_clean'] = 'Unprovoked' #'Swimming/Wading/Standing/Diving etc'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(shark|Shark)', na=False)) & (~sharks['Activity'].str.contains(r'surfmat|bluefish|squid|Fukulya Maru|bathing|capsize|Crossing inlet|trochus|wife|tarpon|dory|Len Bedford|grouper|opihi|menaced|warn bathers|possibly ascended into path', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_clean'] = 'Provoked'\n",
    "sharks.loc[sharks['Activity'] == 'Chasing shark out of bathing area while riding on a surf-ski', 'Activity_clean'] = 'Provoked'\n",
    "\n",
    "sharks.loc[sharks['Activity'].str.contains(r'([Mm]urder|[Ss]uicide)', na=False), 'Activity_clean'] = 'Invalid'\n",
    "\n",
    "sharks.loc[(sharks['Activity'].str.contains('[C|c]anoeing', na=False)) | (sharks['Activity'] == 'Kayaking') | (sharks['Activity'] == 'Kakaying') | (sharks['Activity'] == 'Kayaking ') | (sharks['Activity'] == 'Boat') | (sharks['Activity'] == 'Rowing') | (sharks['Activity'] == 'Sculling') | (sharks['Activity'] == 'Sailing') | (sharks['Activity'] == 'Watercraft') | (sharks['Activity'] == 'Transatlantic Rowing') | (sharks['Activity'] == 'Cruising') | (sharks['Activity'] == 'Rowing') | (sharks['Activity'] == 'Yacht race') | (sharks['Activity'].str.contains('Rowing', na=False)) | (sharks['Name'].str.contains('dinghy|[B|b]oat|ferry|yacht|skiff|cutter')) | (sharks['Activity'].str.contains('dinghy|[B|b]oat|ferry|yacht|skiff|cutter')), 'Activity_clean'] = 'Watercraft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "00efe502-097b-43a9-8d11-50d07c70e062",
    "_uuid": "8cb44720-98fc-4ebc-beb0-87aaba037feb"
   },
   "outputs": [],
   "source": [
    "set(sharks['Activity'][sharks['Activity_clean'] == ''])\n",
    "#sharks.groupby(['Activity', 'Activity_collapsed']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "6341e97a-9bc3-4762-b346-c2724cd0beff",
    "_uuid": "f2be90e6-c1f2-464f-b61b-076cd8d4bc33"
   },
   "outputs": [],
   "source": [
    "sharks['Activity_clean'].unique()\n",
    "#set(sharks['Activity'][sharks['Activity_clean'] == 'Fishing / Proximity to Sea Life'])\n",
    "#set(sharks['Activity'][sharks['Activity_clean'] == 'Provoked'])\n",
    "#set(sharks['Activity'][sharks['Activity_clean'] == 'Sea disaster/capsize/wreck/accident'])\n",
    "#set(sharks['Activity'][sharks['Activity_clean'] == 'Unprovoked']) \n",
    "#set(sharks['Activity'][sharks['Activity_clean'] == 'Watercraft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "ccf41bca-5076-4c70-a3ea-99154053bbd8",
    "_uuid": "c700597e-7ae3-4ce5-ac96-e820e07f477d"
   },
   "outputs": [],
   "source": [
    "#sharks[['Activity', 'Type', 'Name']][(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == False)] \n",
    "\n",
    "#OKAY I am tired of cleaning this data.... went through results of above ^ and final assignations are fine. Gonna trust the few that seem either way from Activity since I am not going through attack pdf info\n",
    "sharks.loc[(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == False), 'Activity_clean'] = sharks.loc[(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == False), 'Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "9cf9e04c-a2a4-4f7c-91dd-de8420a7d263",
    "_uuid": "037477f5-9af6-4902-a5b7-d4a83824302d"
   },
   "outputs": [],
   "source": [
    "#fill nans\n",
    "#going to have to take their work for it.....tempted to drop due to lack of info, but it's not a huge dataset. Then again, that means if they are wrong they will have a larger impact\n",
    "len(sharks[['Activity','Type']][(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == True)])\n",
    "\n",
    "sharks.loc[(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == True), 'Activity_clean'] = sharks.loc[(sharks['Activity_clean'] == '') & (sharks['Activity'].isna() == True), 'Type'] = sharks['Type'].astype(str) + '_NODETAIL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "7e9ccf33-ab3d-4152-8948-d240194b3773",
    "_uuid": "a5a7df71-5533-482f-9a7f-2cc340decb1a"
   },
   "outputs": [],
   "source": [
    "# lets make sure incidents where ppl grabbed sharks are labelled provoked....\n",
    "sharks.loc[sharks['Injury'].str.lower().str.contains(r'(stepped.*|spear.*|dead|netted|shot.*|grabbed|frighten|snared|chasing|snared|kicked|land[ed|ing]|seized|removing|hooked|hit|stepping on|captive|dying|captured|gaffed|boated|fell on|trod on the|secure|roping|poked|harpooned|struck|noosed)(?= shark)', na=False), 'Activity_clean' ] = 'Provoked'\n",
    "\n",
    "sharks.loc[sharks['Injury'].str.lower().str.contains(r'(?<=shark).*(catch|grabbed|capture|trapped|shot|drag|patted|caught|gaff|taken onboard|brought onboard|holding)', na=False), 'Activity_clean'] = 'Provoked'\n",
    "\n",
    "#sharks['Injury'].loc[sharks['Injury'].str.lower().str.contains('provoked', na=False), 'Activity_clean'] = 'Provoked' \n",
    "# do i trust their definition and categorization of provoked???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "f8ad1d69-904f-43f1-bb74-2083a12e2c2b",
    "_uuid": "643de8d9-96aa-4450-a3e0-3cff3a4629b1"
   },
   "outputs": [],
   "source": [
    "#Will explore data further then decide whether to keep.\n",
    "#sharks[['Type', 'Activity']][sharks['Activity_clean'] == 'Provoked']\n",
    "#sharks[['Type', 'Activity']][sharks['Activity_clean'] == 'Questionable']\n",
    "#sharks[['Type', 'Activity']][sharks['Activity_clean'] == 'Unprovoked']\n",
    "#sharks[['Type', 'Activity']][sharks['Activity_clean'] == 'Watercraft']\n",
    "\n",
    "#set(sharks['Activity_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b9eb70d-2423-48c7-98e5-0358f7b696cd",
    "_uuid": "ce787aa2-80e9-4351-bd3c-97dba1e6a055"
   },
   "source": [
    "## Clean 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "34c50148-5584-4278-bc85-79eb665e7fd6",
    "_uuid": "a1267850-8754-41a5-bce2-1192c203bfda"
   },
   "outputs": [],
   "source": [
    "#set(sharks['Area'])\n",
    "#set(sharks['Location'])\n",
    "# set(sharks['Sex '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "eb988842-704b-49ae-b98e-8b601b48cee7",
    "_uuid": "e3b6f42e-014a-41e7-98f5-070f68e625b5"
   },
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "\n",
    "sharks.loc[sharks['Sex '] == 'F', 'Sex_clean'] = 'F'\n",
    "sharks.loc[sharks['Sex '] == 'M', 'Sex_clean'] = 'M'\n",
    "\n",
    "# FEMALE\n",
    "sharks.loc[sharks['Name'].str.contains('female', na=False), 'Sex_clean'] = 'F'\n",
    "\n",
    "# MALE\n",
    "sharks.loc[(sharks['Sex '] == 'M ') | (sharks['Sex '] == 'lli') | (sharks['Name'].str.contains('(?<![F|f]e)male|schoolboy')), 'Sex_clean'] = 'M'\n",
    "sharks.loc[sharks['Name'].str.contains(r'^Frederic|^Jose|^Laurent|^Dr. Leo|^Sergio|^Dr. George|^Ian|^Tony|^Vittorio|^Captain Jack|^Maurice|^William|^John|^Emile|^Jeff|^Miller|^male|^Gilvan|^Shawn|^Stephen|^Yuji|^Josebias|^Ben|^Jurandir|^Nagisa|^Yuji', na=False), 'Sex_clean'] = 'M'\n",
    "\n",
    "# UNKNOWN\n",
    "sharks.loc[(sharks['Name'].isna()) & (sharks['Sex '].isna()), 'Sex_clean'] = 'U' \n",
    "sharks.loc[(sharks['Name'].str.contains(r'^[A-Z]\\.')) | (sharks['Sex '] == '\\.') | (sharks['Name'].str.contains('child|teen|Unknown|Unidentified|Anonymous|student|teacher|aquarist|swimmer|anonymous|a native|a resident|a surfer|U.S. citizen|Fimler|^a youth', na=False)), 'Sex_clean'] = 'U'\n",
    "\n",
    "# MULTI\n",
    "sharks.loc[(sharks['Name'].str.contains(r'[O|o]ccupants|[P|p]assengers|[M|m]igrants|[R|r]efugees|[P|p]oachers|[P|p]eople|[R|r]owers|[C|c]rew|pilgims|Fijians|slaves|youths|passenger|tourists|fishermen|friends|swimmers|men|pilgrims|bathers|\\&|Swimming', na=False)) | (sharks['Sex '] == 'M x 2'), 'Sex_clean'] = 'Multi'\n",
    "sharks.loc[sharks['Case Number'].str.contains('1934.07.11|1959.07.03.a & b', na=False), 'Sex_clean'] = 'Multi' # M&F\n",
    "\n",
    "#SHIPS/NA\n",
    "sharks.loc[(sharks['Activity_clean'] == 'Watercraft') | (sharks['Name'].str.contains(r'dinghy|[B|b]oat|ferry|yacht|skiff|cutter|paddle|launch|Hubeh|Leonida|trawler') & (~sharks['Name'].str.contains(r'[C|c]rew', na=False))), 'Sex_clean'] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "37b1edc2-ddbd-4e4f-85e1-59d1f1548211",
    "_uuid": "f2106714-8fcf-4238-a7d5-bd2dee43363a"
   },
   "outputs": [],
   "source": [
    "print(len(sharks[sharks['Sex_clean'].isna() == True]))\n",
    "set(sharks['Sex_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dcb0670-d5a3-4cdb-a214-e464d3a9c2b4",
    "_uuid": "6519c743-f66e-4045-b83f-6f9cce69d1c7"
   },
   "source": [
    "## Clean 'Species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "d6e21ceb-3373-42cb-9df8-be3a3a386305",
    "_uuid": "e86a145f-92a7-4f19-9e89-59ba5b9a0992"
   },
   "outputs": [],
   "source": [
    "sharks['Species_clean'] = ['' for x in np.arange(len(sharks['Species ']))]\n",
    "sharks = sharks.reset_index()\n",
    "sharks['Species '] = sharks['Species '].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "fd62e320-3f83-40a6-9ad6-7a0b57dd8103",
    "_uuid": "661a9815-2c7c-4cbb-90b6-c7c6c7c0ba5d"
   },
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    if re.search('[S|s]even[\\s|\\-]?gill|7[\\s|\\-]?gill' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] = 'Seven-gill shark'\n",
    "    elif re.search('[T|t]hresher' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] = 'Thresher shark'\n",
    "    elif re.search('[W|w]obbegong' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] = 'Wobbegong shark'\n",
    "    elif re.search('[L|l]emon' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] =  'Lemon shark'\n",
    "    elif re.search('[L|l]eopard' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] =  'Leopard shark'\n",
    "    elif re.search('[H|h]ammerhead' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] =  'Hammerhead shark'\n",
    "    elif re.search('(?<![S|s]and )[T|t]iger' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species_clean'] = 'Tiger shark'\n",
    "    elif re.search('(?<!([G|g]r[e|a]y|[B|b]lue)[\\s|\\-])[N|n]urse' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Nurse shark'\n",
    "    elif re.search('[B|b]lack[\\s|-]?tip' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Blacktip shark'\n",
    "    elif re.search('[S|s]pinner' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Spinner shark'\n",
    "    elif re.search('[D|d]usky' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Dusky shark'\n",
    "    elif re.search('[B|b]ull|[Z|z]ambe[z|s]i|C\\. leucas|Lake Nicaragua' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Bull shark'\n",
    "    elif re.search('[B|b]ronze|(?<![W|w]hite[\\s|\\-]tipped\\s])[W|w]haler|[C|c]opper|[N|n]arrow[\\s|\\-]?tooth' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Copper shark'\n",
    "    elif re.search('[B|b]lack[\\s|-]?fin' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Blackfin shark'\n",
    "    elif re.search('[B|b]lue[\\s|-]?nose|blunt[\\s|\\-]?nose|six[\\s|\\-]?gill|6[\\s|\\-]?gill' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] =  'Bluntnose sixgill shark'\n",
    "    elif re.search('(?<!([[L|l]esser\\s))[W|w]hite |[W|w]hite pointer' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Great white shark'\n",
    "    elif re.search('[M|m]ako|[B|b]lue pointer|[B|b]onit[a|o]' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Mako shark'\n",
    "    elif re.search('[P|p]orbeagle' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Porbeagle shark'\n",
    "    elif re.search('(?<=([G|g]r[e|a]y|[B|b]lue)[\\s|\\-])[N|n]urse|[W|w]hite[\\s|\\-]?tip|[N|n]igano|[S|s]ilver[\\s|\\-]?tip|brown|(?<![B|b]ronze\\s)[W|w]haler|(?<![S|s]potted )[R|r]agged[\\s|\\-]?tooth|[S|s]and[\\s]?bar' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Oceanic whitetip shark'\n",
    "    elif re.search('[D|d]og|mud|piked|dogfish' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Spiny dogfish'\n",
    "    elif re.search('(?<![G|g]r[e|a]y\\s)[R|r]eef' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Caribbean reef shark'\n",
    "    elif re.search('[G|g]r[e|a]y\\s(?!nurse)(?!whaler)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Grey reef shark'\n",
    "    elif re.search('C\\.\\salbimarginatus|[S|s]ilver[\\s|\\-]?tip' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Silvertip shark'\n",
    "    elif re.search('[S|s]ilk|blackspot|[G|g]r[a|e]y\\s(?=whaler)|olive|[R|r]idgeback|[S|s]ickle' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Silky shark'\n",
    "    elif re.search('[G|g]ummy' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Gummy shark'\n",
    "    elif re.search('[C|c]ow' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Cow shark'\n",
    "    elif re.search('[C|c]arpet' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Carpet shark'\n",
    "    elif re.search('[B|b]lue(?![\\-|\\s]{0,1}tip)(?!\\s{1}pointer)(?![\\s|\\-]{1}nurse)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Blue shark'\n",
    "    elif re.search('(?<=[B|b]lue )[N|n]urse|(?<=[G|g]r[e|a]y )[N|n]urse|[S|s]and(?!bar)|(?<=[S|s]potted )[R|r]agged[\\s|\\-]?tooth' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Sand shark'\n",
    "    elif re.search('[B|b]anjo' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Banjo shark - not a real shark'\n",
    "    elif re.search('[S|s]hovelnose' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Shovelnose shark - not a real shark'\n",
    "    elif re.search('[S|s]mooth[\\s|\\-]?hound' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Smooth-hound shark'\n",
    "    elif re.search('[G|g]oblin' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Goblin shark'\n",
    "    elif re.search('[C|c]ookie[\\s|\\-]?cutter|[C|c]igar' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Cookiecutter shark'\n",
    "    elif re.search('[G|g]alapagos' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Galapagos shark'\n",
    "    elif re.search('[C|c]atshark' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Catshark'\n",
    "    elif re.search('[P|p]ort [J|j]ackson' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Port Jackson shark'\n",
    "    elif re.search('[A|a]ngel|[M|m]onkfish' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Angelshark'\n",
    "    elif re.search('[W|w]hale(?<!er)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'Whale shark'\n",
    "    elif re.search('[S|s]oupfin|[S|s]chool|[S|s]napper|[T|t]ope' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species_clean'] = 'School shark'\n",
    "#    elif re.search('' , sharks.loc[x, 'Species ']): \n",
    "#        sharks.loc[x,  'Species'] = ''\n",
    "    elif sharks.loc[x, 'Species '] == 'NA':\n",
    "         sharks.loc[x,  'Species_clean'] = 'No details'\n",
    "    else:\n",
    "        ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "07790341-a5e9-40a6-a735-dc389b4df587",
    "_uuid": "7af15d33-684e-47a4-b2c5-6f4e75105efe"
   },
   "outputs": [],
   "source": [
    "# none are missing :) although some are overlapping (I know some had multiple types of sharks in the string- something to be looked at)\n",
    "sharks.loc[sharks['Species_clean'] == '', 'Species_clean'] = 'No details'\n",
    "\n",
    "set(sharks['Species_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "958afa08-0bd4-4ece-a936-a584a251e10f",
    "_uuid": "c3d4e835-1841-4472-a7a5-fa001253ae69"
   },
   "outputs": [],
   "source": [
    "# Note to my Canadian-metric-self:\n",
    "\n",
    "# ' = feet \n",
    "# \" = inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "59425928-efbe-4f1d-ab28-b01c9190eda6",
    "_uuid": "413baf39-1054-4b89-ba1b-5a1c386fe6d1"
   },
   "outputs": [],
   "source": [
    "sharks['Size_min'] =  [np.nan for x in np.arange(len(sharks['Species ']))]\n",
    "sharks['Size_max'] =  [np.nan for x in np.arange(len(sharks['Species ']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "0b706d4e-6ef6-4da1-9613-6f279a5912b6",
    "_uuid": "3ee97ddd-4779-4ce1-ade0-32e0a8118b2f"
   },
   "outputs": [],
   "source": [
    "sharks[['Species ','Size_min','Size_max']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "80b20ed5-1d6c-4ee9-9612-8c2c3e0c910a",
    "_uuid": "9a742f9f-9f2b-466a-a5e3-55d00639b7e4"
   },
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    #no metres, no inches, has feet\n",
    "    if re.search('[0-9]+\\.?[0-9]?\\s?m' , sharks.loc[x, 'Species ']) is None and re.search('[0-9]\\.?[0-9]?\\'{2}|\\\"{1}' , sharks.loc[x, 'Species ']) is None and bool(re.search('[0-9]\\.?[0-9]?\\'{1}(?!\\')' , sharks.loc[x, 'Species '])):\n",
    "        if len(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_max'] = float(re.search('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species ']).group(1)) * 0.3028\n",
    "        elif len(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])) == 2:\n",
    "            sharks.loc[x,  'Size_min'] = float(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])[0]) * 0.3028\n",
    "            sharks.loc[x,  'Size_max'] = float(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])[1]) * 0.3028\n",
    "        else:\n",
    "            sharks.loc[x,  'Size_max'] = np.nan\n",
    "    #has metres\n",
    "    elif bool(re.search('[0-9]+\\.?[0-9]?\\s?m' , sharks.loc[x, 'Species '])) and (re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])) is None:\n",
    "        if len(re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_max'] = re.search('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species ']).group(1)\n",
    "        elif len(re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])) == 2:\n",
    "            sharks.loc[x,  'Size_min'] = re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])[0]\n",
    "            sharks.loc[x,  'Size_max'] = re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])[1]\n",
    "        else:\n",
    "             sharks.loc[x,  'Size_max'] = np.nan\n",
    "        #first match having no m suffix:\n",
    "    elif len(re.findall('[0-9]+\\.?[0-9]*\\s?m' , sharks.loc[x, 'Species '])) == 1 &  len(re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_min'] = re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])[0]\n",
    "            sharks.loc[x,  'Size_max'] = re.search('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species ']).group(1)\n",
    "    else:\n",
    "        sharks.loc[x,  'Size_max'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "96c8098f-d753-49c6-8d72-560acd349b2d",
    "_uuid": "e7f0f934-a6c8-4531-93c1-f9c169b2ffba"
   },
   "outputs": [],
   "source": [
    "sharks.loc[sharks['Species '] == 'White shark 25 to 3 m', 'Size_max'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2531535a-74f3-4b0c-9597-4f9cbb178a48",
    "_uuid": "a1e753a7-7b96-418b-ba56-35cc3111c823"
   },
   "source": [
    "## Clean 'Injury'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "272b0f37-004d-4f59-96b4-71b87b680c19",
    "_uuid": "090b5905-c0c7-4fcd-a3fa-dd28d1ba0e31"
   },
   "outputs": [],
   "source": [
    "#set(sharks['Injury'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "10b3cb4c-9b32-4173-90ac-f1b5f5a38dd7",
    "_uuid": "75d793e9-34d3-4317-8949-64fc565eaa2e"
   },
   "outputs": [],
   "source": [
    "sharks['Injury_clean'] =  [np.nan for x in np.arange(len(sharks['Species ']))]\n",
    "sharks.loc[sharks['Injury'].str.contains('no injury', na=False), 'Injury_clean'] = 'No injury'\n",
    "sharks.loc[sharks['Injury'].str.lower().str.contains('fatal', na=False), 'Injury_clean'] = 'Fatal'\n",
    "sharks['Injury_clean'] = sharks['Injury_clean'].fillna('Injury')\n",
    "\n",
    "#maybe worth using tokenization to identify limbs vs torso injuries at some point (but i dont want to right now, lets keep moving!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ba1cf64-fe5e-4586-a459-b0707d51a183",
    "_uuid": "3ab49345-c845-4d20-a74a-e53a3666fd53"
   },
   "source": [
    "> ## Clean 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "674e79e3-5cd6-4683-922e-217ca1849dc5",
    "_uuid": "84d58a30-1270-4e54-b391-862bbf1d2c8e"
   },
   "outputs": [],
   "source": [
    "sharks['Date_clean'] =  [np.nan for x in np.arange(len(sharks['Species ']))]\n",
    "sharks['Year_clean'] =  [np.nan for x in np.arange(len(sharks['Species ']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "fe436ab5-7db7-4373-8125-16332d74d4e7",
    "_uuid": "82b016e8-c63e-4f02-8479-34fdc3313faf"
   },
   "outputs": [],
   "source": [
    "sharks = sharks.loc[sharks['Date'] != 'Jun-1018']\n",
    "sharks = sharks.loc[~sharks['Date'].str.contains(r'Before|Fall|Summer|Early [0-9]{4}')]\n",
    "sharks = sharks.reset_index(drop=True)\n",
    "sharks['Date'] = [sharks.loc[x,'Date'].replace('Reported', '') for x in np.arange(len(sharks['Date']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "0f46029d-d213-497f-92ee-a455d9f9f3af",
    "_uuid": "a164a719-32d6-47ba-bee4-2a32b926c88b"
   },
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    if isinstance(sharks.loc[x, 'Date_clean'], datetime.date):\n",
    "        pass\n",
    "    elif bool(re.search(r'[0-9]{2}[- .]{1}[A-z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "        sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{2}.{1}[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0].replace('.', '-')\n",
    "    elif bool(re.search('^[A-z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "        sharks.loc[x, 'Date_clean'] = '15-' + re.findall('^[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('(?<![0-9])[0-9]{1}[- .]{1}[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "        sharks.loc[x, 'Date_clean'] = '0' + re.findall('(?<![0-9])[0-9]{1}.{1}[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('[0-9]{2}[- .]{1}[a-zA-Z]{3}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{2}.{1}[a-zA-Z]{3}', sharks.loc[x, 'Date'])[0]  + '-' + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('[M|m]id.{1}[a-zA-Z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = '15-' + re.findall('[a-zA-Z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif sharks.loc[x, 'Date'].strip() == '26-Sep-t937':\n",
    "         sharks.loc[x, 'Date_clean'] = '26-Sep-1937'\n",
    "    elif sharks.loc[x, 'Date'] == '22-Jul-144':\n",
    "         sharks.loc[x, 'Date_clean'] = '22-Jul-1944'\n",
    "    elif sharks.loc[x, 'Date'] == '02-Ap-2001':\n",
    "         sharks.loc[x, 'Date_clean'] = '02-Apr-2001'\n",
    "    elif sharks.loc[x, 'Date'] == '30-March-1878':\n",
    "         sharks.loc[x, 'Date_clean'] = '30-Mar-1878'\n",
    "    elif bool(re.search('July', sharks.loc[x, 'Date'])):\n",
    "        if bool(re.findall('[0-9]{2}[- .]{1}[A-z]{3}', sharks.loc[x, 'Date'])):\n",
    "            sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{2}.{1}[A-z]{3}', sharks.loc[x, 'Date'])[0] + '-' + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "        else:\n",
    "            sharks.loc[x, 'Date_clean'] = '15-Jul-' + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('December', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = '15-Dec-' + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('Early', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = '01-' + re.findall('[A-z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('Late', sharks.loc[x, 'Date'])):\n",
    "        if bool(re.search('Late [A-z]{3}', sharks.loc[x, 'Date'])):\n",
    "                sharks.loc[x, 'Date_clean'] = '30-' + re.findall('[A-z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "        else:   \n",
    "            sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0][:3] + '9'       \n",
    "    elif bool(re.search('[0-9]{4}.8{1}[0-9]{2}.{1}[0-9]{2}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = [re.findall('(?<![0-9])[0-9]{2}(?!.)', sharks.loc[x, 'Date'])[0] if len(re.findall('(?<![0-9])[0-9]{2}(?!.)', sharks.loc[x, 'Date'])) >=1 else '15'] + [re.search('\\.{1}[0-9]{2}\\.{1}', sharks.loc[x, 'Date']) if re.search('\\.{1}[0-9]{2}\\.{1}', sharks.loc[x, 'Date']) == '.00.' else '.06.'] + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0].split()\n",
    "    elif bool(re.search('[0-9]{2}.{1}[A-z]{3}.{2}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{2}.{1}[A-z]{3}', sharks.loc[x, 'Date'])[0] + re.findall('.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('[0-9]{2}.{2}[A-z]{3}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{2}.{1}', sharks.loc[x, 'Date'])[0] + re.findall('[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('[A-z]{3}[- .]{1}[0-9]{2}[- .]{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "         sharks.loc[x, 'Date_clean'] = re.findall('(?<![0-9])[0-9]{2}.{1}', sharks.loc[x, 'Date'])[0] + re.findall('[A-z]{3}.{1}', sharks.loc[x, 'Date'])[0] + re.findall('[0-9]{4}', sharks.loc[x, 'Date'])[0]\n",
    "    elif bool(re.search('[0-9]{3}[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])):\n",
    "        sharks.loc[x, 'Date_clean'] = re.findall('[0-9]{3}[A-z]{3}', sharks.loc[x, 'Date'])[0][:2] + '-' + re.findall('[A-z]{3}.{1}[0-9]{4}', sharks.loc[x, 'Date'])[0].replace('.', '-')\n",
    "    else:\n",
    "        sharks.loc[x, 'Date_clean'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "52aa310b-958f-4092-ad8e-4f6c110591d2",
    "_uuid": "d5bd7c2a-e799-4605-a8df-67122ec497e0"
   },
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    if len(re.findall('[0-9]{4}', sharks.loc[x, 'Date_clean'])) == 1:\n",
    "        sharks.loc[x, 'Year_clean'] = re.search('[0-9]{4}', sharks.loc[x, 'Date_clean']).group(0)\n",
    "    elif (len(re.findall('[0-9]{4}', sharks.loc[x, 'Date_clean'])) == '') & (len(re.findall('[0-9]{4}', sharks.loc[x, 'Year'].astype('str'))) == 1):\n",
    "        sharks.loc[x, 'Year_clean'] = re.search('[0-9]{4}', sharks.loc[x, 'Year']).group(0)\n",
    "    elif len(re.findall('[0-9]{4}', sharks.loc[x, 'Date'])) >= 1:\n",
    "        sharks.loc[x, 'Year_clean'] = re.search('[0-9]{4}', sharks.loc[x, 'Date']).group(0)\n",
    "    elif len(re.findall('[mid-]?[0-9]{4}s', sharks.loc[x, 'Date'])) >= 1 & (sharks.loc[x, 'Date_clean'] == ''):\n",
    "        sharks.loc[x, 'Year_clean'] = re.search('[0-9]{4}', sharks.loc[x, 'Date']).group(0)[:3] + '5'\n",
    "    elif len(re.findall('Wo[i]?rld War II', sharks.loc[x, 'Date'])) == 1 & (sharks.loc[x, 'Date_clean'] == ''):\n",
    "        sharks.loc[x, 'Year_clean'] = '1940'\n",
    "    else:\n",
    "        sharks.loc[x, 'Year_clean'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "40373d0c-3b35-4d7c-ba28-a5b04fbcc8fc",
    "_uuid": "35738e24-0c0e-4770-b5c0-96fa29a37852"
   },
   "outputs": [],
   "source": [
    "# check occasions where original year and cleaned year do not match for validation\n",
    "\n",
    "sharks.loc[sharks['Date_clean'] == '', 'Date_clean'] = np.nan\n",
    "\n",
    "\n",
    "print(sharks[['Year_clean']][(sharks['Year'].astype(str)) != (sharks['Year_clean'].astype(str))].count())\n",
    "sharks[['Date', 'Date_clean', 'Year', 'Year_clean']][(sharks['Year'].astype(str)) != (sharks['Year_clean'].astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b389b814-bf78-421f-ac12-9f8412fb2ac0",
    "_uuid": "db4c034d-a414-4c4d-9b70-765f587813a3"
   },
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for i in sharks['Date_clean'].astype(str):\n",
    "    if re.search('[A-z]{3}', i)[0] in months:\n",
    "        pass\n",
    "    elif re.search('[A-z]{3}', i)[0] == 'nan':\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b534d832-73e5-4d09-ba0c-83be0b15ac2a",
    "_uuid": "f93099e1-5de6-4930-899b-67fdfd16276b"
   },
   "outputs": [],
   "source": [
    "sharks['Date'][sharks['Date'].str.contains('ate') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "11b6eec4-cc1f-40a7-8c1b-7dd3804c6d5b",
    "_uuid": "b5057cd8-6932-474c-bdf0-d63999312ba8"
   },
   "outputs": [],
   "source": [
    "sharks['Month'] = pd.DatetimeIndex(sharks['Date_clean']).month\n",
    "sharks['Day'] = pd.DatetimeIndex(sharks['Date_clean']).day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "788d3dad-c1e7-4af8-a4f4-8db43e6bd0ee",
    "_uuid": "28941717-767b-4cff-bc65-7ad4fa087d03"
   },
   "source": [
    "## Clean 'Time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "89134203-3962-4142-a71c-679a7d32add1",
    "_uuid": "a1d31af3-093a-480a-b43e-918bea8b9bb6"
   },
   "outputs": [],
   "source": [
    "sharks['Time_clean'] =  [np.nan for x in np.arange(len(sharks['Time']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "28a49a90-09fc-4dcb-9206-c1c99c987386",
    "_uuid": "00e74b1b-2012-432f-9e28-c0c4e4ba15dd"
   },
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    if len(re.findall('[0-9]{2}h[0-9]{2}', str(sharks.loc[x, 'Time']))) != 0:\n",
    "        sharks.loc[x, 'Time_clean'] = re.sub('h', ':', re.search('[0-9]{2}h[0-9]{2}', sharks.loc[x, 'Time']).group(0))\n",
    "    else:\n",
    "        sharks.loc[x, 'Time_clean'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "f3d4f3f4-177c-4955-8dc1-84286d627b8a",
    "_uuid": "19a913af-e0b2-4e6b-8505-850a2180607e"
   },
   "outputs": [],
   "source": [
    "sharks.to_excel('sharks_clean.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "acbaaedc-0577-4238-a5db-699fc41f47a7",
    "_uuid": "2af6b71e-ec19-45a4-90fd-87c96f9c5cb6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
