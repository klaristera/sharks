{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "import re\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = pd.read_excel('C:/Users/klari/Documents/Learning/a Datasets/GSAF5 (sharks).xls', sort =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Sex_clean</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020.05.09</td>\n",
       "      <td>09-May-2020</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Sand Dollar Beach, Santa Cruz County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Ben Kelly</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>White shark</td>\n",
       "      <td>R. Collier, GSAF, K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2020.05.09-Kelly.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2020.05.09</td>\n",
       "      <td>2020.05.09</td>\n",
       "      <td>6523.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020.05.08</td>\n",
       "      <td>08-May-2020</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Southside Beach, near Geelong</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Dylan Nacass</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2020.05.08-Nacass.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2020.05.08</td>\n",
       "      <td>2020.05.08</td>\n",
       "      <td>6522.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020.05.01</td>\n",
       "      <td>01-May-2020</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Summerland, Santa Barbara County</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Mandy Boyd</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>5' to 6' shark</td>\n",
       "      <td>R. Collier, GSAF, K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2020.05.01-Boyd.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2020.05.01</td>\n",
       "      <td>2020.05.01</td>\n",
       "      <td>6521.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020.04.29</td>\n",
       "      <td>29-Apr-2020</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Moonlight Beach, San Diego County</td>\n",
       "      <td>Body Boarding</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K. McMurray, TrackingSharks.com and M. Michael...</td>\n",
       "      <td>2020.04.29-Encinitas.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2020.04.29</td>\n",
       "      <td>2020.04.29</td>\n",
       "      <td>6520.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020.04.21</td>\n",
       "      <td>21-Apr-2020</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Killick Creek near Crescent Head</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>female</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B. Myatt, GSAF &amp; K. McMurray, TrackingSharks.com</td>\n",
       "      <td>2020.04.21-CrescentHead.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2020.04.21</td>\n",
       "      <td>2020.04.21</td>\n",
       "      <td>6519.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number         Date    Year        Type    Country             Area  \\\n",
       "0  2020.05.09  09-May-2020  2020.0  Unprovoked        USA       California   \n",
       "1  2020.05.08  08-May-2020  2020.0  Unprovoked  AUSTRALIA         Victoria   \n",
       "2  2020.05.01  01-May-2020  2020.0  Unprovoked        USA       California   \n",
       "3  2020.04.29  29-Apr-2020  2020.0         NaN        USA       California   \n",
       "4  2020.04.21  21-Apr-2020  2020.0  Unprovoked  AUSTRALIA  New South Wales   \n",
       "\n",
       "                               Location       Activity          Name Sex   \\\n",
       "0  Sand Dollar Beach, Santa Cruz County        Surfing     Ben Kelly    M   \n",
       "1         Southside Beach, near Geelong        Surfing  Dylan Nacass    M   \n",
       "2      Summerland, Santa Barbara County       Swimming    Mandy Boyd    F   \n",
       "3     Moonlight Beach, San Diego County  Body Boarding          male    M   \n",
       "4      Killick Creek near Crescent Head       Swimming        female    F   \n",
       "\n",
       "   ...        Species                              Investigator or Source  \\\n",
       "0  ...     White shark  R. Collier, GSAF, K. McMurray, TrackingSharks.com   \n",
       "1  ...             NaN                    K. McMurray, TrackingSharks.com   \n",
       "2  ...  5' to 6' shark  R. Collier, GSAF, K. McMurray, TrackingSharks.com   \n",
       "3  ...             NaN  K. McMurray, TrackingSharks.com and M. Michael...   \n",
       "4  ...             NaN   B. Myatt, GSAF & K. McMurray, TrackingSharks.com   \n",
       "\n",
       "                           pdf  \\\n",
       "0         2020.05.09-Kelly.pdf   \n",
       "1        2020.05.08-Nacass.pdf   \n",
       "2          2020.05.01-Boyd.pdf   \n",
       "3     2020.04.29-Encinitas.pdf   \n",
       "4  2020.04.21-CrescentHead.pdf   \n",
       "\n",
       "                                        href formula  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                href Case Number.1  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...    2020.05.09   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...    2020.05.08   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...    2020.05.01   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...    2020.04.29   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2020.04.21   \n",
       "\n",
       "  Case Number.2 original order Sex_clean Unnamed: 23  \n",
       "0    2020.05.09         6523.0       NaN         NaN  \n",
       "1    2020.05.08         6522.0       NaN         NaN  \n",
       "2    2020.05.01         6521.0       NaN         NaN  \n",
       "3    2020.04.29         6520.0       NaN         NaN  \n",
       "4    2020.04.21         6519.0       NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks.dropna(subset = [\"Date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Sex_clean',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = sharks.drop(columns = [ 'Unnamed: 23', 'href formula'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(sharks.Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks['Country2'] = [str(x).strip().upper() for x in sharks['Country']]    \n",
    "sharks['Country'] = sharks['Country2']\n",
    "sharks = sharks.drop(columns = ['Country2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharks[sharks['Country'].str.contains(r'.+\\?')]\n",
    "sharks['Country'] = sharks['Country'].str.replace('?', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks.loc[sharks['Country'] == 'COAST OF AFRICA', 'Country'] = 'AFRICA'\n",
    "sharks.loc[sharks['Country'] == 'CEYLON (SRI LANKA)', 'Country'] = 'CEYLON'\n",
    "sharks.loc[sharks['Country'] == 'REUNION', 'Country'] = 'REUNION ISLAND'\n",
    "sharks.loc[sharks['Country'] == 'MALDIVES', 'Country'] = 'MALDIVE ISLANDS'\n",
    "sharks.loc[sharks['Country'] == 'UNITED ARAB EMIRATES (UAE)', 'Country'] = 'UNITED ARAB EMIRATES'\n",
    "sharks.loc[sharks['Country'] == 'ST HELENA, British overseas territory', 'Country'] = 'ST. HELENA'\n",
    "sharks.loc[sharks['Country'] == 'COLUMBIA', 'Country'] = 'COLOMBIA'\n",
    "sharks.loc[sharks['Country'] == 'SOUTHWEST PACIFIC OCEAN', 'Country'] = 'SOUTH PACIFIC OCEAN'\n",
    "sharks.loc[sharks['Country'] == 'FEDERATED STATES OF MICRONESIA', 'Country'] = 'MICRONESIA'\n",
    "sharks.loc[sharks['Country'] == 'TOBAGO', 'Country'] = 'TRINIDAD & TOBAGO'\n",
    "sharks.loc[sharks['Country'] == 'WESTERN SAMOA', 'Country'] = 'SAMOA'\n",
    "sharks.loc[sharks['Country'] == 'MID-PACIFC OCEAN', 'Country'] = 'MID PACIFIC OCEAN' \n",
    "sharks.loc[sharks['Country'] == 'SOLOMON ISLANDS', 'Country'] = 'SOLOMON ISLANDS / VANUATU'\n",
    "sharks.loc[sharks['Country'] == 'VANUATU', 'Country'] = 'SOLOMON ISLANDS / VANUATU'\n",
    "sharks.loc[sharks['Country'] == 'RED SEA / INDIAN OCEAN', 'Country'] = 'RED SEA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Year'].between(1, 1000, inclusive=True) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Date'].str.contains(r'.+B\\.C\\.') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks['Year2'] = sharks['Date'].str.extract(r'([0-9]{4})')\n",
    "sharks.loc[sharks['Year']==0, 'Year'] = sharks['Year2']\n",
    "\n",
    "#some of the years extracted above are approximated. ie, 'Before 1905' is too vague to estimate; I've taken 1905 as the year. This could vary in degree of incorrectness- actual may be 1904 or 1704. \n",
    "#possibly should be dropped altogether- will decide when histogram of year is viewed\n",
    "\n",
    "sharks.loc[sharks['Year'].isna() == True, 'Year'] = sharks['Year2']\n",
    "sharks = sharks.drop(columns = ['Year2'])\n",
    "sharks.loc[sharks['Date'] == 'World War II', 'Year'] = '1940'\n",
    "sharks['Year'] = sharks['Year'].replace(\"'\", '')\n",
    "sharks = sharks[sharks['Year'].isna() == False]\n",
    "sharks['Year'] = sharks['Year'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharks['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unprovoked', nan, 'Provoked', 'Questionable', 'Watercraft',\n",
       "       'Unverified', 'Invalid', 'Under investigation', 'Sea Disaster',\n",
       "       'Unconfirmed', 'Boat'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Case Number'] != '2020.04.29']\n",
    "sharks['Type'][sharks['Type'].isna() == True] = 'Unprovoked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks = sharks[sharks['Type'] != 'Invalid'] #if invalid, will only confuse data\n",
    "sharks.loc[sharks['Type'] == 'Boat', 'Type'] = 'Watercraft' #was going to remove these, but will actually switch to 'Watercraft' for consistency\n",
    "sharks = sharks[sharks['Type'] != 'Under investigation'] # there is only one so just going to drop it\n",
    "sharks = sharks[sharks['Type'] != 'Unconfirmed'] # there is only one so just going to drop it- missing much info so don't want to coerce into unprovoked\n",
    "sharks.loc[(sharks['Type'].isna() == True) & (sharks['Activity'] == 'Wreck of a sampam'), 'Type'] = 'Sea Disaster'\n",
    "sharks.loc[sharks['Type'] == 'Unverified', 'Type'] = 'Unprovoked' #only 1; acting as if legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(sharks['Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#set(sharks['Activity'][(sharks['Activity'].str.contains(r'(swim|stand|wading|Swim|Stand|Wading)')) & (~sharks['Activity'].str.contains(r'(Fish|fish|apsize|shark|Shark|porpoise|oyster|whale|dolphin|perlemoen|abalone|slaughterhouse)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False))])\n",
    "sharks['Activity_collapsed'] = ''\n",
    "# includes: (Attempt|Attack|harpooned|dead|Catching|Chumming|captured|force-feed|Kiss|feeding|photographing|Dragging|filming|Feeding|Feeling|Filming|Finning|Fell|Fishing|Gaffing|Grab|Hand feed|Harass|Harpoon|Hauling|Help|Hoist|Hold|Hunt|Inspect|Investigat|landed on|Killing|fishing|Measuring|Moving|Netting|Observing|Petting|Photographing|Picking|Pulling|foot|hand|Removing|Restrain|Reviv|catch|Shooting|Grab|Slap|lasso|pulled|shot|Spearing|Standing|stepped|Stuffing|feeding|Swimming with|Tagging|Teasing|Testing|Thrashing|Touching|netted|Watching|Wrangling|)\n",
    "\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(collided|[C|c]apsize|aground|[S|s]hipwreck|210-ton brig|[W|w]reck|[D|d]isaster|[A|a]ccident|founder|crash|overturn|stove|sank)', na=False)) | (sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_collapsed'] = 'Sea disaster/capsize/wreck/accident'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'( fish|[F|f]ish|[C|c]lam|porpoise|whale|dolphin|perlemoen|[A|a]balone|crayfish|slaughterhouse|bait|[T|t]rochus|opihi|squid|shad|bluefish|net|[C|c]rab|conch|beche-de-mer|trepang|tuna|shellfish|[L|l]obster|[S|s]hrimp|[P|p]rawn|meat|turtle|seal|[O|o]yster|[A|a]ngling|sardine|bichiques|[C|c]hum|[S|s]almon|[F|f]lounder|sea urchins|sponges)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)) & (~sharks['Activity'].str.contains(r'Hawaiian brig|Observing', na=False)), 'Activity_collapsed'] = 'Fishing / Proximity to Sea Life'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(SUP|[S|s]wim|[S|s]ta[m|n]d|[W|w]ad[e|ing]|[B|b]athing|[D|d]iv[e|ing]|[S|s]urf|[W|w]alk|[S|s]it|[B|b]oard|Playing|Snorkeling|Floating|[F|f]eet|Cling|Clean|Lying|Adrift|Kneeling|Holding|Paddling|Splash|Wash|[F|f]ell|Crouch|banana|Filming$|documentary|Jump|Rest|Sitting|Tread|[S|s]ki)')) & (~sharks['Activity'].str.contains(r'(Chasing|[C|c]apsize|aground|[S|s]hipwreck|210-ton brig|[W|w]reck|[D|d]isaster|[A|a]ccident|feeding| fish|[F|f]ish|[C|c]lam|porpoise|oyster|whale|dolphin|perlemoen|[A|a]balone|crayfish|slaughterhouse|bait|trochus|opihi|squid|bluefish|[C|c]rab|conch|beche-de-mer|trepang|tuna|shellfish|[L|l]obster|[S|s]hrimp|meat|turtle|seal|[O|o]yster|[C|c]hum|[S|s]almon|sea urchins|sponges|Feeding|holding shark|putting hand|shark tank|stepped on|grabbed shark)', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_collapsed'] = 'Unprovoked' #'Swimming/Wading/Standing/Diving etc'\n",
    "sharks.loc[(sharks['Activity'].str.contains(r'(shark|Shark)', na=False)) & (~sharks['Activity'].str.contains(r'surfmat|bluefish|squid|Fukulya Maru|bathing|capsize|Crossing inlet|trochus|wife|tarpon|dory|Len Bedford|grouper|opihi|menaced|warn bathers|possibly ascended into path', na=False)) & (~sharks['Type'].str.contains('Sea Disaster', na=False)), 'Activity_collapsed'] = 'Provoked'\n",
    "sharks.loc[sharks['Activity'] == 'Chasing shark out of bathing area while riding on a surf-ski', 'Activity_collapsed'] = 'Provoked'\n",
    "\n",
    "\n",
    "\n",
    "sharks.loc[(sharks['Activity'].str.contains('[C|c]anoeing', na=False)) | (sharks['Activity'] == 'Kayaking') | (sharks['Activity'] == 'Kakaying') | (sharks['Activity'] == 'Kayaking ') | (sharks['Activity'] == 'Boat') | (sharks['Activity'] == 'Rowing') | (sharks['Activity'] == 'Sculling') | (sharks['Activity'] == 'Sailing') | (sharks['Activity'] == 'Watercraft') | (sharks['Activity'] == 'Transatlantic Rowing') | (sharks['Activity'] == 'Cruising') | (sharks['Activity'] == 'Rowing') | (sharks['Activity'] == 'Yacht race') | (sharks['Activity'].str.contains('Rowing', na=False)) | (sharks['Name'].str.contains('dinghy|[B|b]oat|ferry|yacht|skiff|cutter')), 'Activity_collapsed'] = 'Watercraft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'   ',\n",
       " ' a canoe was pursuing a schooner that had forcibily abducted 5 young girls',\n",
       " '\"Climbing up to ship after repairing the stern in water\"',\n",
       " '\"Crossing the river\"',\n",
       " '3 men & 2 boys picked up wearing life jackets and with inner tube',\n",
       " 'Attempting to fix motor',\n",
       " 'Attempting to rescue drowning man',\n",
       " 'Attempting to rescue shipmate',\n",
       " 'Attempting to retreive a dinghy',\n",
       " 'Attempting to set underwater endurance record',\n",
       " 'Batin',\n",
       " 'Bending over',\n",
       " 'Boat stopped to repair electric pump',\n",
       " 'Boat swamped',\n",
       " 'Boating',\n",
       " 'Body found on deserted luxury yacht, 38’ Christine',\n",
       " 'Canoe swamped',\n",
       " 'Climbing back on ship',\n",
       " 'Collecting aquarium specimens',\n",
       " 'Collecting marine specimens',\n",
       " 'Collecting shells',\n",
       " 'Coming ashore on a hawser',\n",
       " 'Conducting research',\n",
       " 'Crossing inlet in a boat, seen fighting sharks with his oar, sharks smashed boat',\n",
       " 'Crossing river on a raft',\n",
       " 'Crossing the bay at the ford',\n",
       " 'Crossing the river mouth',\n",
       " \"Days before the surrender of Singapore, the 3 men escaped to Sumatra where they acquired a 17' dinghy. After 125 days at sea they drifted back to Sumatra\",\n",
       " 'Defecating in water beneath the docks',\n",
       " 'Deserting the bark Nazarene',\n",
       " 'Dry shelling',\n",
       " 'Escaping from Alacatraz',\n",
       " 'Escaping from blackbirding vessel',\n",
       " 'Exercising his dog in the shallows',\n",
       " 'Filming underwater, carrying powerhead',\n",
       " 'Fleeing across a river',\n",
       " 'Gathering shells',\n",
       " 'Hilo',\n",
       " 'In boat being towed by ship, Karnak',\n",
       " 'In deep water about 100 yards from his ship',\n",
       " 'In rubber dinghy with Captain Eddie Rickenbacker for 21 days. ',\n",
       " 'In waist-deep water',\n",
       " 'Inflatable boat',\n",
       " 'Knocked into the water',\n",
       " 'Launching a boat',\n",
       " 'Leaving the water',\n",
       " 'Lifesaving drill',\n",
       " 'Lifesaving exhibition',\n",
       " 'Murder',\n",
       " 'Murdered',\n",
       " 'Murdered by Thai pirates',\n",
       " 'NSB Meshing',\n",
       " 'On a float',\n",
       " 'On life raft tethered to lifeboat. A seaman put hand over side to rinse a cup',\n",
       " 'Painting a ship',\n",
       " 'Parachuted into Pacific',\n",
       " 'Pleasure boating',\n",
       " 'Pulling raft out to ride to shore',\n",
       " 'Ran into the water',\n",
       " 'Reaching for life preserver',\n",
       " 'Rescuing',\n",
       " 'Rescuing seaman after ship sunk by German raider',\n",
       " 'Riding a horse',\n",
       " 'Riding floatation device',\n",
       " 'Riding horseback across the creek',\n",
       " 'Rolled off raft',\n",
       " 'Row boat (from the gunboat Elcano) was sinking, put finger in hole',\n",
       " \"Scientific research (Dr. Sonny Gruber's student)\",\n",
       " 'Searching for remains of  Dr. Marais',\n",
       " 'Sleeping in anchored boat',\n",
       " 'Steinhart Aquarium',\n",
       " 'Suicide',\n",
       " 'Swept off deck of S.S.Frontenac enroute from West Indies to US',\n",
       " 'Taking wife to beach & about 1 m from the shore',\n",
       " 'Towing rubber dinghy',\n",
       " 'Trailing hand in the water',\n",
       " 'Trying to catch a wounded bird',\n",
       " 'Underwater photography',\n",
       " 'Unknown, but it was said to be the \"First known attack in Sydney Harbour\"',\n",
       " 'Went to aid of child being menaced by the shark',\n",
       " 'boat from the Austrian ship Elizabeth',\n",
       " nan}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sharks['Activity'][sharks['Activity_collapsed'] == ''])\n",
    "#sharks.groupby(['Activity', 'Activity_collapsed']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unprovoked', '', 'Fishing / Proximity to Sea Life', 'Watercraft',\n",
       "       'Provoked', 'Sea disaster/capsize/wreck/accident'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks['Activity_collapsed'].unique()\n",
    "#set(sharks['Activity'][sharks['Activity_collapsed'] == 'Fishing / Proximity to Sea Life'])\n",
    "#set(sharks['Activity'][sharks['Activity_collapsed'] == 'Provoked'])\n",
    "#set(sharks['Activity'][sharks['Activity_collapsed'] == 'Sea disaster/capsize/wreck/accident'])\n",
    "#set(sharks['Activity'][sharks['Activity_collapsed'] == 'Unprovoked']) \n",
    "#set(sharks['Activity'][sharks['Activity_collapsed'] == 'Watercraft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks[['Activity', 'Type']][(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == False)] \n",
    "\n",
    "#OKAY I am real tired of cleaning this data.... \n",
    "sharks.loc[(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == False), 'Activity_collapsed'] = sharks.loc[(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == False), 'Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nans\n",
    "#just going to have to take their work for it.....tempted to drop due to lack of info, but it's not a huge dataset. Then again, that means if they are wrong they will have a larger impact\n",
    "len(sharks[['Activity','Type']][(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == True)])\n",
    "\n",
    "sharks.loc[(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == True), 'Activity_collapsed'] = sharks.loc[(sharks['Activity_collapsed'] == '') & (sharks['Activity'].isna() == True), 'Type'] = sharks['Type'].astype(str) + '_TEMP'#'TempAssign'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fishing / Proximity to Sea Life',\n",
       " 'Provoked',\n",
       " 'Provoked_TEMP',\n",
       " 'Questionable_TEMP',\n",
       " 'Sea disaster/capsize/wreck/accident',\n",
       " 'Unprovoked',\n",
       " 'Unprovoked_TEMP',\n",
       " 'Watercraft',\n",
       " 'Watercraft_TEMP'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Will explore data further then decide whether to keep.\n",
    "sharks[['Activity_collapsed', 'Activity']][sharks['Type'] == 'Provoked']\n",
    "#sharks[['Activity_collapsed', 'Activity']][sharks['Type'] == 'Questionable']\n",
    "#sharks[['Activity_collapsed', 'Activity']][sharks['Type'] == 'Unprovoked']\n",
    "#sharks[['Activity_collapsed', 'Activity']][sharks['Type'] == 'Watercraft']\n",
    "\n",
    "set(sharks['Activity_collapsed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', 'F', 'M', 'M ', 'M x 2', 'N', 'lli', nan}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set(sharks['Area'])\n",
    "#set(sharks['Location'])\n",
    "set(sharks['Sex '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "\n",
    "sharks.loc[sharks['Sex '] == 'F', 'Sex_clean'] = 'F'\n",
    "sharks.loc[sharks['Sex '] == 'M', 'Sex_clean'] = 'M'\n",
    "\n",
    "# FEMALE\n",
    "sharks.loc[sharks['Name'].str.contains('female', na=False), 'Sex_clean'] = 'F'\n",
    "\n",
    "# MALE\n",
    "sharks.loc[(sharks['Sex '] == 'M ') | (sharks['Sex '] == 'lli') | (sharks['Name'].str.contains('(?<![F|f]e)male|schoolboy')), 'Sex_clean'] = 'M'\n",
    "sharks.loc[sharks['Name'].str.contains(r'^Frederic|^Jose|^Laurent|^Dr. Leo|^Sergio|^Dr. George|^Ian|^Tony|^Vittorio|^Captain Jack|^Maurice|^William|^John|^Emile|^Jeff|^Miller|^male|^Gilvan|^Shawn|^Stephen|^Yuji|^Josebias|^Ben|^Jurandir|^Nagisa|^Yuji', na=False), 'Sex_clean'] = 'M'\n",
    "\n",
    "# UNKNOWN\n",
    "sharks.loc[(sharks['Name'].isna()) & (sharks['Sex '].isna()), 'Sex_clean'] = 'U' \n",
    "sharks.loc[(sharks['Name'].str.contains(r'^[A-Z]\\.')) | (sharks['Sex '] == '\\.') | (sharks['Name'].str.contains('child|teen|Unknown|Unidentified|Anonymous|student|teacher|aquarist|swimmer|anonymous|a native|a resident|a surfer|U.S. citizen|Fimler|^a youth', na=False)), 'Sex_clean'] = 'U'\n",
    "\n",
    "# MULTI\n",
    "sharks.loc[(sharks['Name'].str.contains(r'[O|o]ccupants|[P|p]assengers|[M|m]igrants|[R|r]efugees|[P|p]oachers|[P|p]eople|[R|r]owers|[C|c]rew|pilgims|Fijians|slaves|youths|passenger|tourists|fishermen|friends|swimmers|men|pilgrims|bathers|\\&|Swimming', na=False)) | (sharks['Sex '] == 'M x 2'), 'Sex_clean'] = 'Multi'\n",
    "sharks.loc[sharks['Case Number'].str.contains('1934.07.11|1959.07.03.a & b', na=False), 'Sex_clean'] = 'Multi' # M&F\n",
    "\n",
    "#SHIPS/NA\n",
    "sharks.loc[(sharks['Activity_collapsed'] == 'Watercraft') | (sharks['Name'].str.contains(r'dinghy|[B|b]oat|ferry|yacht|skiff|cutter|paddle|launch|Hubeh|Leonida|trawler') & (~sharks['Name'].str.contains(r'[C|c]rew', na=False))), 'Sex_clean'] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'F', 'M', 'Multi', 'N/A', 'U', nan}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sharks[sharks['Sex_clean'].isna() == True]))\n",
    "set(sharks['Sex_clean'])\n",
    "\n",
    "#sharks[sharks['Sex '].isna() == True] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href', 'Case Number.1',\n",
       "       'Case Number.2', 'original order', 'Sex_clean', 'Activity_collapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 'Species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks['Species'] = ['' for x in np.arange(len(sharks['Species ']))]\n",
    "sharks = sharks.reset_index()\n",
    "sharks['Species '] = sharks['Species '].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: Possible nested set at position 6\n"
     ]
    }
   ],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    if re.search('[S|s]even[\\s|\\-]?gill|7[\\s|\\-]?gill' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] = 'Seven-gill shark'\n",
    "    elif re.search('[T|t]hresher' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] = 'Thresher shark'\n",
    "    elif re.search('[W|w]obbegong' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] = 'Wobbegong shark'\n",
    "    elif re.search('[L|l]emon' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] =  'Lemon shark'\n",
    "    elif re.search('[L|l]eopard' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] =  'Leopard shark'\n",
    "    elif re.search('[H|h]ammerhead' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] =  'Hammerhead shark'\n",
    "    elif re.search('(?<![S|s]and )[T|t]iger' , sharks.loc[x, 'Species ']):\n",
    "        sharks.loc[x,  'Species'] = 'Tiger shark'\n",
    "    elif re.search('(?<!([G|g]r[e|a]y|[B|b]lue)[\\s|\\-])[N|n]urse' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Nurse shark'\n",
    "    elif re.search('[B|b]lack[\\s|-]?tip' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Blacktip shark'\n",
    "    elif re.search('[S|s]pinner' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Spinner shark'\n",
    "    elif re.search('[D|d]usky' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Dusky shark'\n",
    "    elif re.search('[B|b]ull|[Z|z]ambe[z|s]i|C. leucas|Lake Nicaragua' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Bull shark'\n",
    "    elif re.search('[B|b]ronze|(?<![W|w]hite[\\s|\\-]tipped\\s])[W|w]haler|[C|c]opper|[N|n]arrow[\\s|\\-]?tooth' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Copper shark'\n",
    "    elif re.search('[B|b]lack[\\s|-]?fin' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Blackfin shark'\n",
    "    elif re.search('[B|b]lue[\\s|-]?nose|blunt[\\s|\\-]?nose|six[\\s|\\-]?gill|6[\\s|\\-]?gill' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] =  'Bluntnose sixgill shark'\n",
    "    elif re.search('(?<!([[L|l]esser\\s))[W|w]hite |[W|w]hite pointer' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Great white shark'\n",
    "    elif re.search('[M|m]ako|[B|b]lue pointer|[B|b]onit[a|o]' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Mako shark'\n",
    "    elif re.search('[P|p]orbeagle' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Porbeagle shark'\n",
    "    elif re.search('(?<=([G|g]r[e|a]y|[B|b]lue)[\\s|\\-])[N|n]urse|[W|w]hite[\\s|\\-]?tip|[N|n]igano|[S|s]ilver[\\s|\\-]?tip|brown|(?<![B|b]ronze\\s)[W|w]haler|(?<![S|s]potted )[R|r]agged[\\s|\\-]?tooth|[S|s]and[\\s]?bar' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Oceanic whitetip shark'\n",
    "    elif re.search('[D|d]og|mud|piked|dogfish' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Spiny dogfish'\n",
    "    elif re.search('(?<![G|g]r[e|a]y\\s)[R|r]eef' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Caribbean reef shark'\n",
    "    elif re.search('[G|g]r[e|a]y\\s(?!nurse)(?!whaler)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Grey reef shark'\n",
    "    elif re.search('C\\.\\salbimarginatus|[S|s]ilver[\\s|\\-]?tip' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Silvertip shark'\n",
    "    elif re.search('[S|s]ilk|blackspot|[G|g]r[a|e]y\\s(?=whaler)|olive|[R|r]idgeback|[S|s]ickle' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Silky shark'\n",
    "    elif re.search('[G|g]ummy' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Gummy shark'\n",
    "    elif re.search('[C|c]ow' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Cow shark'\n",
    "    elif re.search('[C|c]arpet' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Carpet shark'\n",
    "    elif re.search('[B|b]lue(?![\\-|\\s]{0,1}tip)(?!\\s{1}pointer)(?![\\s|\\-]{1}nurse)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Blue shark'\n",
    "    elif re.search('(?<=[B|b]lue )[N|n]urse|(?<=[G|g]r[e|a]y )[N|n]urse|[S|s]and(?!bar)|(?<=[S|s]potted )[R|r]agged[\\s|\\-]?tooth' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Sand shark'\n",
    "    elif re.search('[B|b]anjo' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Banjo shark - not a real shark'\n",
    "    elif re.search('[S|s]hovelnose' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Shovelnose shark - not a real shark'\n",
    "    elif re.search('[S|s]mooth[\\s|\\-]?hound' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Smooth-hound shark'\n",
    "    elif re.search('[G|g]oblin' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Goblin shark'\n",
    "    elif re.search('[C|c]ookie[\\s|\\-]?cutter|[C|c]igar' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Cookiecutter shark'\n",
    "    elif re.search('[G|g]alapagos' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Galapagos shark'\n",
    "    elif re.search('[C|c]atshark' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Catshark'\n",
    "    elif re.search('[P|p]ort [J|j]ackson' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Port Jackson shark'\n",
    "    elif re.search('[A|a]ngel|[M|m]onkfish' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Angelshark'\n",
    "    elif re.search('[W|w]hale(?<!er)' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'Whale shark'\n",
    "    elif re.search('[S|s]oupfin|[S|s]chool|[S|s]napper|[T|t]ope' , sharks.loc[x, 'Species ']): \n",
    "        sharks.loc[x,  'Species'] = 'School shark'\n",
    "#    elif re.search('' , sharks.loc[x, 'Species ']): \n",
    "#        sharks.loc[x,  'Species'] = ''\n",
    "    elif sharks.loc[x, 'Species '] == 'NA':\n",
    "         sharks.loc[x,  'Species'] = 'No details'\n",
    "    else:\n",
    "        ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'Angelshark',\n",
       " 'Banjo shark - not a real shark',\n",
       " 'Blackfin shark',\n",
       " 'Blacktip shark',\n",
       " 'Blue shark',\n",
       " 'Bluntnose sixgill shark',\n",
       " 'Bull shark',\n",
       " 'Caribbean reef shark',\n",
       " 'Carpet shark',\n",
       " 'Catshark',\n",
       " 'Cookiecutter shark',\n",
       " 'Copper shark',\n",
       " 'Cow shark',\n",
       " 'Dusky shark',\n",
       " 'Galapagos shark',\n",
       " 'Goblin shark',\n",
       " 'Great white shark',\n",
       " 'Grey reef shark',\n",
       " 'Gummy shark',\n",
       " 'Hammerhead shark',\n",
       " 'Lemon shark',\n",
       " 'Leopard shark',\n",
       " 'Mako shark',\n",
       " 'No details',\n",
       " 'Nurse shark',\n",
       " 'Oceanic whitetip shark',\n",
       " 'Porbeagle shark',\n",
       " 'Port Jackson shark',\n",
       " 'Sand shark',\n",
       " 'School shark',\n",
       " 'Seven-gill shark',\n",
       " 'Shovelnose shark - not a real shark',\n",
       " 'Silky shark',\n",
       " 'Silvertip shark',\n",
       " 'Smooth-hound shark',\n",
       " 'Spinner shark',\n",
       " 'Spiny dogfish',\n",
       " 'Thresher shark',\n",
       " 'Tiger shark',\n",
       " 'Whale shark',\n",
       " 'Wobbegong shark'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# none are missing :) although some are overlapping (I know some had multiple types of sharks in the string- something to be looked at)\n",
    "set(sharks['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharks['Species '] = sharks['Species '].str.replace('\\\\','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(sharks['Species '][(sharks['Species'] == 'No details') | (sharks['Species'] == '')].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to my Canadian-metric-self:\n",
    "\n",
    "# ' = feet \n",
    "# \" = inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks['Size_min'] =  ['' for x in np.arange(len(sharks['Species ']))]\n",
    "sharks['Size_max'] =  ['' for x in np.arange(len(sharks['Species ']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Size_min</th>\n",
       "      <th>Size_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White shark</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Species  Size_min Size_max\n",
       "0  White shark               NaN"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharks[['Species ','Size_min','Size_max']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.arange(len(sharks)):\n",
    "    #no metres, no inches, has feet\n",
    "    if re.search('[0-9]+\\.?[0-9]?\\s?m' , sharks.loc[x, 'Species ']) is None and re.search('[0-9]\\.?[0-9]?\\'{2}|\\\"{1}' , sharks.loc[x, 'Species ']) is None and bool(re.search('[0-9]\\.?[0-9]?\\'{1}(?!\\')' , sharks.loc[x, 'Species '])):\n",
    "        if len(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_max'] = float(re.search('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species ']).group(1)) #* 0.3028\n",
    "        elif len(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])) == 2:\n",
    "            sharks.loc[x,  'Size_min'] = float(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])[0]) #* 0.3028\n",
    "            sharks.loc[x,  'Size_max'] = float(re.findall('([0-9]\\.?[0-9]?)\\'{1}(?!\\')' , sharks.loc[x, 'Species '])[1]) #* 0.3028\n",
    "        else:\n",
    "            sharks.loc[x,  'Size_max'] = np.nan\n",
    "    #has metres\n",
    "    elif bool(re.search('[0-9]+\\.?[0-9]?\\s?m' , sharks.loc[x, 'Species '])) and (re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])) is None:\n",
    "        if len(re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_max'] = re.search('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species ']).group(1)\n",
    "        elif len(re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])) == 2:\n",
    "            sharks.loc[x,  'Size_min'] = re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])[0]\n",
    "            sharks.loc[x,  'Size_max'] = re.findall('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species '])[1]\n",
    "        else:\n",
    "             sharks.loc[x,  'Size_max'] = np.nan\n",
    "        #first match having no m suffix:\n",
    "    elif len(re.findall('[0-9]+\\.?[0-9]*\\s?m' , sharks.loc[x, 'Species '])) == 1 &  len(re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])) == 1:\n",
    "            sharks.loc[x,  'Size_min'] = re.findall('([0-9]+\\.?[0-9]?)\\s?(?=[\\-|to])' , sharks.loc[x, 'Species '])[0]\n",
    "            sharks.loc[x,  'Size_max'] = re.search('([0-9]+\\.?[0-9]?)\\s?m' , sharks.loc[x, 'Species ']).group(1)\n",
    "    else:\n",
    "        sharks.loc[x,  'Size_max'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks.loc[sharks['Species '] == 'White shark 25 to 3 m', 'Size_max'] = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
